{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "\n",
    "#sample dataset from paper\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets - Books and DVD taken from https://www.cs.jhu.edu/~mdredze/datasets/sentiment/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 x 400\n"
     ]
    }
   ],
   "source": [
    "#datasets used in paper\n",
    "\n",
    "data_path_books = \"processed_acl/books/\"\n",
    "data_path_dvd = \"processed_acl/dvd/\"\n",
    "\n",
    "def Load_Dataset(path_data):\n",
    "    \n",
    "    random.seed(1)\n",
    "   \n",
    "    pos = []\n",
    "    neg = []\n",
    "    \n",
    "    def get_words(line):\n",
    "\n",
    "        z = [tuple(x.split(':')) for x in re.findall('\\w*?:\\d', line)]\n",
    "        z = ' '.join([' '.join([x[0]] * int(x[1])) for x in z if '_' not in x[0]])\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    for line in open(os.path.join(path_data, 'negative.review')):\n",
    "        neg.append(get_words(line))\n",
    "    for line in open(os.path.join(path_data, 'positive.review')):\n",
    "        pos.append(get_words(line))\n",
    "        \n",
    "    random.shuffle(pos)\n",
    "    random.shuffle(neg)\n",
    "    \n",
    "    split_pos = int(len(pos) * .8)\n",
    "    split_neg = int(len(neg) * .8)\n",
    "    \n",
    "    train_data = pos[:split_pos] + neg[:split_neg]\n",
    "    test_data = pos[split_pos:] + neg[split_neg:]\n",
    "    \n",
    "    train_labels = [1] * len(pos[:split_pos]) + [0] * len(neg[:split_neg])\n",
    "    test_labels = [1] * len(pos[split_pos:]) + [0] * len(neg[split_neg:])\n",
    "    \n",
    "    return train_data, np.array(train_labels), test_data, np.array(test_labels), ['neg', 'pos']\n",
    "\n",
    "\n",
    "train_data, train_labels, test_data, test_labels, class_names = Load_Dataset(data_path_books)\n",
    "\n",
    "assert len(train_data) == 1600\n",
    "assert len(test_data) == 400\n",
    "\n",
    "print(len(train_data), \"x\", len(test_data))\n",
    "#pd.DataFrame(train_data)\n",
    "#train_data[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm for LIME\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"algorithm_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use similarity kernel = $\\pi_x (z) = exp(-D(x,z)^2/\\sigma^2$) for all experiments\n",
    "\n",
    "D is cosine distance for text\n",
    "\n",
    "g $\\in$ G, where G is a class of potentially interpretable models (i.e linear models, decision trees, etc), choose g to be linear model\n",
    "\n",
    "$\\xi$(x) = argmin L(f, g, $\\pi_x$) + $\\Omega$(g)\n",
    "\n",
    "$L(f, g, \\pi_x) = \\sum_{z, z' \\in Z} \\pi_x(z) (f(z) - g(z'))^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def log(s):\n",
    "    if DEBUG:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainer - LIME\n",
    "\n",
    "#f = classifier, N = number of samples, pi = similarity kernel, K = length of explanation (used for K-Lasso)\n",
    "#x = instance, x' = interpretable version (bag of words), z' = subspace of x, Z = perturbed X, w = weights \n",
    "\n",
    "class LIME:\n",
    "    \n",
    "    #required f - classifier\n",
    "    #required pi - similarity kernel\n",
    "    #required N - number of samples\n",
    "    #required K - length of explanation K\n",
    "    \n",
    "    def __init__(self, N, K=10):\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        #sigma = 25\n",
    "        #self.kernel_fn = lambda D: np.sqrt(np.exp(-(D**2) / sigma ** 2))\n",
    "\n",
    "    def similarity_kernel(self, x, z):\n",
    "        \n",
    "        # cosine similarity\n",
    "        cos_distance = sklearn.metrics.pairwise.cosine_distances(x, z)[0]*100\n",
    "        \n",
    "        # distance kernel\n",
    "        sigma = 25\n",
    "        kernalized_distance = lambda D: np.sqrt(np.exp(-(D**2)/sigma**2))\n",
    "        kernalized_D = kernalized_distance(cos_distance)\n",
    "        \n",
    "        return kernalized_D\n",
    "    \n",
    "    def explain_instance(self, x, label, classifier):   \n",
    "        \n",
    "        #x, #x', #z, #z'\n",
    "        \n",
    "        # x -> x'\n",
    "        #print(x)\n",
    "        \n",
    "        # Z <- {}\n",
    "        \n",
    "        #gets all the features\n",
    "        features = x.nonzero()[1]\n",
    "        vals = np.array(x[x.nonzero()])[0]\n",
    "        log([\"values\", vals])\n",
    "\n",
    "        #gets a sample\n",
    "        num_features = len(features)\n",
    "        sample = np.random.randint(1, num_features, self.N - 1)\n",
    "        log([\"sample\", sample])\n",
    "        \n",
    "        #create sample x features\n",
    "        data = np.zeros((self.N, num_features))\n",
    "        inverse_data = np.zeros((self.N, num_features))     \n",
    "        \n",
    "        #setup\n",
    "        data[0] = np.ones(num_features)\n",
    "        inverse_data[0] = vals\n",
    "        features_range = range(len(features))\n",
    "        \n",
    "        # z' = sample around (x')\n",
    "        #for i in range(N):\n",
    "        for i, s in enumerate(sample, start=1):\n",
    "            #drawing non zero elements of x' uniformly at random\n",
    "            #(where the number of such draws is also uniformly sampled)\n",
    "            \n",
    "            #random permutation with features_range\n",
    "            active = np.random.choice(features_range, s, replace=False) \n",
    "            \n",
    "            data[i, active] = 1\n",
    "            \n",
    "            for j in active:\n",
    "                inverse_data[i, j] = 1\n",
    "                \n",
    "        log([\"data\", data])\n",
    "        log([\"inverse_data\", inverse_data])\n",
    "        \n",
    "        #converting back to sparse matrix\n",
    "        z_s = sp.sparse.lil_matrix((inverse_data.shape[0], x.shape[1]))\n",
    "        z_s[:, features] = inverse_data\n",
    "        z_s = sp.sparse.csr_matrix(z_s)\n",
    "        log([\"sparse_inverse\", z_s.getnnz()])\n",
    "        log([\"sparse_inverse\", z_s])\n",
    "        \n",
    "        \n",
    "        # Z = Z union <z'_i f(z_i), pi_x(z_i)>\n",
    "        \n",
    "        #f(z) - getting the labels\n",
    "        labels = classifier.predict_proba(z_s)\n",
    "        \n",
    "        #pi_x(z_i) - getting the distances\n",
    "        #weights the data based on distance\n",
    "        orig_x = z_s[0]\n",
    "        weights = self.similarity_kernel(orig_x, z_s)\n",
    "        log([\"distances:\", weights])        \n",
    "        weighted_data = data * weights[:, np.newaxis]\n",
    "        \n",
    "        #label = 0 | 1\n",
    "        log([\"labels\", labels])\n",
    "        mean = np.mean(labels[:, label])\n",
    "        shifted_labels = labels[:, label] - mean\n",
    "        log(['mean', mean])\n",
    "        \n",
    "        weighted_labels = shifted_labels * weights\n",
    "        \n",
    "        log([\"weighted_data\", weighted_data])\n",
    "        log([\"weighted_labels\", weighted_labels])\n",
    "\n",
    "        \n",
    "        used_features = range(weighted_data.shape[1])\n",
    "        log([\"used_features\", len(used_features)])\n",
    "        \n",
    "        #K-Lasso\n",
    "        #w = K-Lasso(Z,K) with z'_i as features, f(z) as target\n",
    "        #selecting K features with Lasso (using the regularization path [9 - Least Angle Regression]) \n",
    "        \n",
    "        nonzero = used_features\n",
    "        \n",
    "        alpha = 1\n",
    "        alphas, _, coefs = sklearn.linear_model.lars_path(weighted_data, weighted_labels, method='lasso')\n",
    "        \n",
    "        #select K\n",
    "        for i in range(len(coefs.T) - 1, 0, -1):\n",
    "            nonzero = coefs.T[i].nonzero()[0]\n",
    "            if len(nonzero) <= self.K:\n",
    "                chosen_coefs = coefs.T[i]\n",
    "                alpha = alphas[i]\n",
    "                break\n",
    "                \n",
    "        used_features = nonzero\n",
    "\n",
    "        #learning the weights via least squares        \n",
    "        #fitting a linear sparse model with target labels of f(z)\n",
    "        local_model = linear_model.Ridge(alpha=0, fit_intercept=False)\n",
    "        local_model.fit(weighted_data[:, used_features], weighted_labels)\n",
    "        \n",
    "        original_label = labels[0, label]\n",
    "        local_prediction = local_model.predict(data[0, used_features].reshape(1, -1)) + mean\n",
    "        log([\"True Label vs Local Model Prediction: \", original_label, local_prediction])\n",
    "        \n",
    "        #print(zip(used_features, local_model.coef_), key=lambda x:np.abs(x[1]))\n",
    "        \n",
    "        #mapping feature indices back to features\n",
    "        vocab_features = []\n",
    "        \n",
    "        for feature in used_features:\n",
    "            vocab_features.append(features[feature])\n",
    "            \n",
    "        #print(used_features)\n",
    "        explanations = sorted(zip(vocab_features, local_model.coef_), key=lambda x:np.abs(x[1]), reverse=True)\n",
    "        #mapped_explanations = map(lambda x: \n",
    "                                  \n",
    "        log([\"raw explanation:\", explanations])\n",
    "        \n",
    "        #x => x' => z' => z\n",
    "        #mapping features back to vocab\n",
    "        for explanation in explanations:\n",
    "            log([\"explanation:\", inverse_vocabulary[explanation[0]], explanation[1]])\n",
    "            \n",
    "        return explanations, mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  23035\n",
      "vocab ['00' '000' '0001' ... 'zombie' 'zur' 'zus']\n",
      "train_vectors shape:  (1079, 23035)\n",
      "instance_vector shape:  (1, 23035)\n",
      "test prediction: [[0.00248503 0.99751497]]\n",
      "[(22865, 1), (9072, 1), (17304, 1), (946, 1), (21507, 1), (17234, 1), (2515, 1), (18764, 1), (12257, 1), (15766, 1)]\n",
      "From: creps@lateran.ucs.indiana.edu (Stephen A. Creps)\n",
      "Subject: Re: The doctrine of Original Sin\n",
      "Organization: Indiana University\n",
      "Lines: 31\n",
      "\n",
      "In article <May.11.02.39.02.1993.28325@athos.rutgers.edu> Eugene.Bigelow@ebay.sun.com writes:\n",
      ">>This all obviously applies equally well to infants or adults, since\n",
      ">>both have souls.  Infants must be baptized, therefore, or they cannot\n",
      ">>enter into Heaven.  They too need this form of life in them, or they\n",
      ">>cannot enter into Heaven.\n",
      ">\n",
      ">Are you saying that baptism has nothing to do with asking Jesus to come into\n",
      ">your heart and accepting him as your savior, but is just a ritual that we\n",
      ">must go through to enable us to enter Heaven?\n",
      "\n",
      "   I don't think Joe was saying any such thing.  However, your question\n",
      "on \"asking Jesus to come into your heart\" seems to imply that infants\n",
      "are not allowed to have Christ in theirs.  Why must Baptism always be\n",
      "viewed by some people as a sort of \"prodigal son\" type of thing; i.e. a\n",
      "sudden change of heart, going from not accepting Christ to suddenly\n",
      "accepting Christ?  Why can't people start out with Christ from shortly\n",
      "after birth, and build their relationship from there?  After all, does\n",
      "a man suddenly meet a woman, and then marry her that same day?  From my\n",
      "experiences, I've learned that all relationships must be built,\n",
      "including one's relationship with God.\n",
      "\n",
      "   Also Joe is speaking from the standpoint that Baptism is not just a\n",
      "ritual, but that through it God bestows sacramental grace upon the\n",
      "recipient.  Certainly for those with the mental faculties to know Christ\n",
      "it is necessary to believe in Him.  However, the Sacrament itself\n",
      "bestows grace on the recipient, and makes a permanent mark of adoption\n",
      "into God's family on the soul.\n",
      "\n",
      "-\t-\t-\t-\t-\t-\t-\t-\t-\t-\n",
      "Steve Creps, Indiana University\n",
      "creps@lateran.ucs.indiana.edu\n",
      "\n",
      "['values', array([1, 1, 1])]\n",
      "['sample', array([2, 2, 2, 1])]\n",
      "['data', array([[1., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 0., 1.]])]\n",
      "['inverse_data', array([[1., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 1., 1.],\n",
      "       [0., 0., 1.]])]\n",
      "['sparse_inverse', 10]\n",
      "['sparse_inverse', <5x23035 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 10 stored elements in Compressed Sparse Row format>]\n",
      "['distances:', array([1.        , 0.7638468 , 0.7638468 , 0.7638468 , 0.23953341])]\n",
      "['labels', array([[0.09986209, 0.90013791],\n",
      "       [0.09788637, 0.90211363],\n",
      "       [0.09788637, 0.90211363],\n",
      "       [0.09788637, 0.90211363],\n",
      "       [0.09815501, 0.90184499]])]\n",
      "['mean', 0.9016647580957218]\n",
      "['weighted_data', array([[1.        , 1.        , 1.        ],\n",
      "       [0.        , 0.7638468 , 0.7638468 ],\n",
      "       [0.        , 0.7638468 , 0.7638468 ],\n",
      "       [0.        , 0.7638468 , 0.7638468 ],\n",
      "       [0.        , 0.        , 0.23953341]])]\n",
      "['weighted_labels', array([-1.52685104e-03,  3.42870917e-04,  3.42870917e-04,  3.42870917e-04,\n",
      "        4.31709243e-05])]\n",
      "['used_features', 3]\n",
      "['True Label vs Local Model Prediction: ', 0.9001379070560722, array([0.90013791])]\n",
      "['raw explanation:', [(13948, -0.00197572497383236), (15343, 0.00026864469708232113), (22719, 0.00018022923710048192)]]\n",
      "['explanation:', 'going', -0.00197572497383236]\n",
      "['explanation:', 'is', 0.00026864469708232113]\n",
      "['explanation:', 'what', 0.00018022923710048192]\n",
      "([(13948, -0.00197572497383236), (15343, 0.00026864469708232113), (22719, 0.00018022923710048192)], 0.9016647580957218)\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "\n",
    "# vecotorize - bag of words\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True) \n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "#vocab\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"vocab size: \", len(vocab))\n",
    "\n",
    "#creating indices for retrieving vocab\n",
    "#print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "terms = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "indices = np.array(list(vectorizer.vocabulary_.values()))\n",
    "inverse_vocabulary = terms[np.argsort(indices)]\n",
    "print(\"vocab\", inverse_vocabulary)\n",
    "\n",
    "#print(vectorizer)\n",
    "print(\"train_vectors shape: \", train_vectors.shape)\n",
    "\n",
    "#print(\"instance vector\", test_vectors[10])\n",
    "print(\"instance_vector shape: \", test_vectors[10].shape)\n",
    "\n",
    "\n",
    "# build classifier\n",
    "# l1 = lasso, l2 = ridge\n",
    "classifier = sklearn.linear_model.LogisticRegression(fit_intercept=True)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "print(\"test prediction:\", classifier.predict_proba(test_vectors[10]))\n",
    "\n",
    "random_explainer = RandomExplainer(K=10)\n",
    "random_explanation = random_explainer.explain_instance(test_vectors[10], test_labels[10])\n",
    "print(random_explanation)\n",
    "\n",
    "# inject easy sample for testing\n",
    "print(test_data[10])\n",
    "test_data[10] = \"what is what is going what going\"\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "LIME_explainer = LIME(N=5, K=3)\n",
    "\n",
    "LIME_explain_fn = LIME_explainer.explain_instance(test_vectors[10], test_labels[10], classifier)\n",
    "print(LIME_explain_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Explainer for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explainer - Random\n",
    "class RandomExplainer:\n",
    "    def __init__(self, K=10):\n",
    "        self.K = K\n",
    "\n",
    "    def explain_instance(self, instance_vector, label):\n",
    "        nonzero = instance_vector.nonzero()[1]\n",
    "        explanation = np.random.choice(nonzero, self.K)\n",
    "        return [(x, 1) for x in explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13948, 1),\n",
       " (13948, 1),\n",
       " (15343, 1),\n",
       " (22719, 1),\n",
       " (15343, 1),\n",
       " (13948, 1),\n",
       " (22719, 1),\n",
       " (13948, 1),\n",
       " (13948, 1),\n",
       " (13948, 1)]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = RandomExplainer(K=10)\n",
    "explain_fn = explainer.explain_instance\n",
    "\n",
    "# vecotorize - bag of words\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True) \n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# build classifier\n",
    "# l1 = lasso, l2 = ridge\n",
    "classifier = sklearn.linear_model.LogisticRegression(fit_intercept=True)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "explain_fn(test_vectors[10], test_labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of classifiers used in the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classifiers\n",
    "def get_classifier(name):\n",
    "    #lasso\n",
    "    if name == 'logreg-l1':\n",
    "        return linear_model.LogisticRegression(penalty='l1', fit_intercept=True)\n",
    "    #ridge\n",
    "    if name == 'logreg-l2':\n",
    "        return linear_model.LogisticRegression(fit_intercept=True)\n",
    "    if name == 'svm':\n",
    "        return svm.SVC(probability=True, kernel='rbf', C=10,gamma=0.001)\n",
    "    if name == 'tree':\n",
    "        return tree.DecisionTreeClassifier(random_state=1)\n",
    "    if name == 'neighbors':\n",
    "        return neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce Figure 2. Paper didn't provide the data for figure 2, found it through matching the text in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 262 example entry from paper: From: pauld@verdix.com (Paul Durbin)\n",
      "Subject: Re: DAVID CORESH IS! GOD!\n",
      "Nntp-Posting-Host: sarge.hq.verdix.com\n",
      "Organization: Verdix Corp\n",
      "Lines: 8\n",
      "\n",
      "On one of the morning shows (I think is was the Today Show), David Koresh's\n",
      "lawyer was interviewed. During that interview he flipped through some letters\n",
      "that David Koresh wrote. On one of letters was written in Hebrew (near the\n",
      "bottom of the page):\n",
      "\n",
      "  koresh adonai\n",
      "\n",
      "Did anyone else see that? What could this mean by him (David) writing this?\n",
      "\n",
      "explanation: Posting 0.2720876102700741\n",
      "explanation: Host 0.25948766301957604\n",
      "explanation: Re 0.16622922117966074\n",
      "explanation: Nntp 0.10509618653018112\n",
      "explanation: in -0.0855246455583565\n",
      "explanation: by -0.08063167244703102\n",
      "explanation: this -0.0791801845091335\n",
      "explanation: anyone -0.058164891954772306\n",
      "explanation: was -0.05102401547209193\n",
      "explanation: Paul -0.046171209084104915\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#print(sklearn.datasets.fetch_20newsgroups().keys())\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']\n",
    "train_data = newsgroups_train.data\n",
    "train_labels = newsgroups_train.target\n",
    "test_data = newsgroups_test.data\n",
    "test_labels = newsgroups_test.target\n",
    "\n",
    "#finds sample from paper\n",
    "def find_sample():\n",
    "    for i in range(len(test_data)):\n",
    "        #print(test_sample)\n",
    "        if \"pauld@verdix.com\" in test_data[i]:\n",
    "            print(\"id\", i, \"example entry from paper:\", test_data[i])\n",
    "            return i, test_data[i]\n",
    "        \n",
    "test_sample = find_sample()\n",
    "\n",
    "# vecotorize - bag of words\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True) \n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# indexes vocab\n",
    "terms = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "indices = np.array(list(vectorizer.vocabulary_.values()))\n",
    "inverse_vocabulary = terms[np.argsort(indices)]\n",
    "\n",
    "# build classifier\n",
    "# l1 = lasso, l2 = ridge\n",
    "classifier = svm.SVC(probability=True, kernel='rbf', C=10, gamma=0.001)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "LIME_explainer = LIME(N=15000, K=10)\n",
    "\n",
    "#Look for the example provided in the paper figure 2\n",
    "LIME_explain_fn = LIME_explainer.explain_instance(test_vectors[262], test_labels[262], classifier)\n",
    "explanations, mean = LIME_explain_fn\n",
    "\n",
    "features = []\n",
    "coefficients = []\n",
    "\n",
    "#mapping it back to vocab\n",
    "for explanation in explanations:\n",
    "    print(\"explanation:\", inverse_vocabulary[explanation[0]], explanation[1])\n",
    "    features.append(inverse_vocabulary[explanation[0]])\n",
    "    coefficients.append(explanation[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2720876102700741, 0.25948766301957604, 0.16622922117966074, 0.10509618653018112, 0.0855246455583565, 0.08063167244703102, 0.0791801845091335, 0.058164891954772306, 0.05102401547209193, 0.046171209084104915]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot figure 2\n",
    "feature_val = np.arange(len(features))\n",
    "class_color = []\n",
    "abs_coef = list(map(abs, coefficients))\n",
    "\n",
    "print(abs_coef)\n",
    "\n",
    "for coef in coefficients:\n",
    "    if coef >= 0:\n",
    "        class_color.append(\"purple\")\n",
    "    else:\n",
    "        class_color.append(\"green\")\n",
    "        \n",
    "plt.barh(feature_val, abs_coef, color=class_color, align='center', alpha=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('coefficients')\n",
    "plt.title('Words that A2 Considers Important')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure_2.png\" alt=\"Figure_2\" style=\"width: 250px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 (Section 5.2 - Are explanations faithful to the model?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "def get_vectorizer(train_data, test_data):\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, binary=True) \n",
    "    train_vectors = vectorizer.fit_transform(train_data)\n",
    "    test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "    return train_vectors, test_vectors, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "#ground truth features from logistic regression\n",
    "\n",
    "#train_data, train_labels, test_data, test_labels, class_names = Load_Dataset(data_path_books)\n",
    "books = data_path_books\n",
    "DVD = data_path_dvd\n",
    "\n",
    "def experiment_1(data_path, classifier_name):\n",
    "\n",
    "    train_data, train_labels, test_data, test_labels, class_names = Load_Dataset(data_path)\n",
    "\n",
    "    train_vectors, test_vectors, vectorizer = get_vectorizer(train_data, test_data)\n",
    "    \n",
    "    terms = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "    indices = np.array(list(vectorizer.vocabulary_.values()))\n",
    "    inverse_vocabulary = terms[np.argsort(indices)]\n",
    "    \n",
    "    classifier = get_classifier(classifier_name)\n",
    "    classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "    results_LIME = []\n",
    "    results_Random = []\n",
    "    \n",
    "    LIME_explainer = LIME(N=15000, K=10)\n",
    "    Random_explainer = RandomExplainer(K=10)\n",
    "\n",
    "    coef_features = classifier.coef_.nonzero()[1]\n",
    "\n",
    "    for i in tqdm(range(len(test_data))):\n",
    "        \n",
    "        true_features = set([x for x in test_vectors[i].nonzero()[1] if x in coef_features])\n",
    "        \n",
    "        random_features = set(map(lambda x:x[0], Random_explainer.explain_instance(test_vectors[i], test_labels[i])))\n",
    "        lime_features = set(map(lambda x:x[0], LIME_explainer.explain_instance(test_vectors[i], test_labels[i], classifier)[0]))\n",
    "        \n",
    "        log([\"true\", true_features])\n",
    "        log([\"lime\", lime_features])\n",
    "        \n",
    "        results_LIME.append(float(len(true_features.intersection(lime_features))) / len(true_features))\n",
    "        results_Random.append(float(len(true_features.intersection(random_features))) / len(true_features))\n",
    "        \n",
    "    return results_Random, results_LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400/400 [06:32<00:00,  1.02it/s]\n",
      "100%|| 400/400 [06:42<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "results_Random_books, results_LIME_books = experiment_1(books, \"logreg-l2\")\n",
    "results_Random_DVD, results_LIME_DVD = experiment_1(DVD, \"logreg-l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "random 0.19879846131468262\n",
      "LIME 0.23083406078661473\n",
      "DVDs:\n",
      "random 0.2084580928286352\n",
      "LIME 0.24668181371476197\n"
     ]
    }
   ],
   "source": [
    "print(\"Books:\")\n",
    "print(\"random\", np.mean(results_Random_books))\n",
    "print(\"LIME\", np.mean(results_LIME_books))\n",
    "\n",
    "print(\"DVDs:\")\n",
    "print(\"random\", np.mean(results_Random_DVD))\n",
    "print(\"LIME\", np.mean(results_LIME_DVD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2 (Section 5.3 - Should I trust this prediction?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def experiment_2(data_path, classifier):\n",
    "    \n",
    "    percent_untrustworthy = .25\n",
    "    \n",
    "    train_data, train_labels, test_data, test_labels, class_names = Load_Dataset(data_path)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, binary=True) \n",
    "    train_vectors = vectorizer.fit_transform(train_data)\n",
    "    test_vectors = vectorizer.transform(test_data)\n",
    "    \n",
    "    terms = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "    indices = np.array(list(vectorizer.vocabulary_.values()))\n",
    "    inverse_vocabulary = terms[np.argsort(indices)]\n",
    "    \n",
    "    classifier = get_classifier(classifier)\n",
    "    classifier.fit(train_vectors, train_labels)\n",
    "    \n",
    "    np.random.seed(1)    \n",
    "    untrustworthy_rounds = []\n",
    "    all_features = range(train_vectors.shape[1])\n",
    "    num_untrustworthy = int(train_vectors.shape[1] * percent_untrustworthy)\n",
    "    \n",
    "    for _ in range(100):\n",
    "        untrustworthy_rounds.append(np.random.choice(all_features, num_untrustworthy, replace=False))\n",
    "  \n",
    "    LIME_explainer = LIME(N=15000, K=10)\n",
    "    RANDOM_explainer = RandomExplainer(K=10)\n",
    "    exps = {}\n",
    "    \n",
    "    explainer_names = ['LIME', 'RANDOM']\n",
    "    \n",
    "    for expl in explainer_names:\n",
    "        exps[expl] = []\n",
    "\n",
    "    predictions = classifier.predict(test_vectors)\n",
    "    predict_probas = classifier.predict_proba(test_vectors)[:,1]\n",
    "  \n",
    "    for i in tqdm(range(test_vectors.shape[0])):\n",
    "        \n",
    "        exp, mean = LIME_explainer.explain_instance(test_vectors[i], 1, classifier)\n",
    "        exps['LIME'].append((exp, mean))\n",
    "\n",
    "        exp = RANDOM_explainer.explain_instance(test_vectors[i], 1)\n",
    "        exps['RANDOM'].append(exp)\n",
    "\n",
    "    log(exps['RANDOM'])\n",
    "\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "  \n",
    "    for name in explainer_names:\n",
    "        \n",
    "        precision[name] = []\n",
    "        recall[name] = []\n",
    "        f1[name] = []\n",
    "        flipped_preds_size = []\n",
    "  \n",
    "    for untrustworthy in untrustworthy_rounds:\n",
    "        \n",
    "        t = test_vectors.copy()\n",
    "        t[:, untrustworthy] = 0\n",
    "        mistrust_idx = np.argwhere(classifier.predict(t) != classifier.predict(test_vectors)).flatten()\n",
    "        \n",
    "        log(['Number of suspect predictions', len(mistrust_idx)])\n",
    "        shouldnt_trust = set(mistrust_idx)\n",
    "        flipped_preds_size.append(len(shouldnt_trust))\n",
    "        mistrust = collections.defaultdict(lambda:set())\n",
    "        trust = collections.defaultdict(lambda: set())\n",
    "        \n",
    "        trust_fn = lambda prev, curr: (prev > 0.5 and curr > 0.5) or (prev <= 0.5 and curr <= 0.5)\n",
    "        trust_fn_all = lambda exp, unt: len([x[0] for x in exp if x[0] in unt]) == 0\n",
    "    \n",
    "        for i in range(test_vectors.shape[0]):\n",
    "      \n",
    "            exp, mean = exps['LIME'][i]\n",
    "            prev_tot = predict_probas[i]\n",
    "            prev_tot2 = sum([x[1] for x in exp]) + mean\n",
    "            tot = prev_tot2 - sum([x[1] for x in exp if x[0] in untrustworthy])\n",
    "            trust['LIME'].add(i) if trust_fn(tot, prev_tot) else mistrust['LIME'].add(i)\n",
    "      \n",
    "            exp = exps['RANDOM'][i]\n",
    "            trust['RANDOM'].add(i) if trust_fn_all(exp, untrustworthy) else mistrust['RANDOM'].add(i)\n",
    "\n",
    "        for expl in explainer_names:\n",
    "            # switching the definition\n",
    "            false_positives = set(trust[expl]).intersection(shouldnt_trust)\n",
    "            true_positives = set(trust[expl]).difference(shouldnt_trust)\n",
    "            false_negatives = set(mistrust[expl]).difference(shouldnt_trust)\n",
    "            true_negatives = set(mistrust[expl]).intersection(shouldnt_trust)\n",
    "\n",
    "            try:\n",
    "                prec= len(true_positives) / float(len(true_positives) + len(false_positives))\n",
    "            except:\n",
    "                prec= 0\n",
    "            try:\n",
    "                rec= float(len(true_positives)) / (len(true_positives) + len(false_negatives))\n",
    "            except:\n",
    "                rec= 0\n",
    "\n",
    "            precision[expl].append(prec)\n",
    "            recall[expl].append(rec)\n",
    "            f1z = 2 * (prec * rec) / (prec + rec) if (prec and rec) else 0\n",
    "            f1[expl].append(f1z)\n",
    "    \n",
    "    log(['Average number of flipped predictions:', np.mean(flipped_preds_size), '+-', np.std(flipped_preds_size)])\n",
    "    log(['Precision:'])\n",
    "    for expl in explainer_names:\n",
    "        log([expl, np.mean(precision[expl]), '+-', np.std(precision[expl]), 'pvalue', sp.stats.ttest_ind(precision[expl], precision['LIME'])[1].round(4)])\n",
    "    log(['Recall:'])\n",
    "    for expl in explainer_names:\n",
    "        log([expl, np.mean(recall[expl]), '+-', np.std(recall[expl]), 'pvalue', sp.stats.ttest_ind(recall[expl], recall['LIME'])[1].round(4)])\n",
    "    log(['F1:'])\n",
    "    for expl in explainer_names:\n",
    "        log([expl, np.mean(f1[expl]), '+-', np.std(f1[expl]), 'pvalue', sp.stats.ttest_ind(f1[expl], f1['LIME'])[1].round(4)])\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision, recall, f1 = experiment_2(books, \"logreg-l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg-l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400/400 [06:18<00:00,  1.06it/s]\n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|| 400/400 [06:22<00:00,  1.05it/s]\n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400/400 [13:22<00:00,  2.01s/it]\n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|| 400/400 [13:38<00:00,  2.05s/it]\n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400/400 [55:50<00:00,  8.38s/it] \n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|| 400/400 [50:05<00:00,  7.51s/it] \n",
      "/Users/Ru/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "scores_books = {\"logreg-l2\":[], \"neighbors\":[], \"svm\": []}\n",
    "scores_dvds = {\"logreg-l2\":[], \"neighbors\":[], \"svm\": []}\n",
    "\n",
    "for cat in scores_books:\n",
    "    print(cat)\n",
    "    scores_books[cat] = experiment_2(books, cat)\n",
    "    scores_dvds[cat] = experiment_2(DVD, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "logreg-l2-LIME 0.9665532502679051\n",
      "logreg-l2-Random 0.13777789040152993\n",
      "nn-LIME 0.9414930780390607\n",
      "nn-Random 0.1447327043562891\n",
      "svm-LIME 0.9668618358252699\n",
      "svm-Random 0.14168659390499347\n"
     ]
    }
   ],
   "source": [
    "print(\"Books:\")\n",
    "\n",
    "print(\"logreg-l2-LIME\", np.mean(scores_books['logreg-l2'][2]['LIME']))\n",
    "print(\"logreg-l2-Random\", np.mean(scores_books['logreg-l2'][2]['RANDOM']))\n",
    "      \n",
    "print(\"nn-LIME\", np.mean(scores_books['neighbors'][2]['LIME']))\n",
    "print(\"nn-Random\", np.mean(scores_books['neighbors'][2]['RANDOM']))\n",
    "      \n",
    "print(\"svm-LIME\", np.mean(scores_books['svm'][2]['LIME']))\n",
    "print(\"svm-Random\", np.mean(scores_books['svm'][2]['RANDOM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVDs:\n",
      "logreg-l2-LIME 0.9690990484594824\n",
      "logreg-l2-Random 0.1459032642884926\n",
      "nn-LIME 0.907823857014562\n",
      "nn-Random 0.1501628088210605\n",
      "svm-LIME 0.9671570187764498\n",
      "svm-Random 0.1466569550965759\n"
     ]
    }
   ],
   "source": [
    "print(\"DVDs:\")\n",
    "\n",
    "print(\"logreg-l2-LIME\", np.mean(scores_dvds['logreg-l2'][2]['LIME']))\n",
    "print(\"logreg-l2-Random\", np.mean(scores_dvds['logreg-l2'][2]['RANDOM']))\n",
    "      \n",
    "print(\"nn-LIME\", np.mean(scores_dvds['neighbors'][2]['LIME']))\n",
    "print(\"nn-Random\", np.mean(scores_dvds['neighbors'][2]['RANDOM']))\n",
    "      \n",
    "print(\"svm-LIME\", np.mean(scores_dvds['svm'][2]['LIME']))\n",
    "print(\"svm-Random\", np.mean(scores_dvds['svm'][2]['RANDOM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
